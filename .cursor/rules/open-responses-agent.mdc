---
description: Open Responses Agent Development - Build autonomous agents with the unified HuggingFace router
globs:
  - "**/*.ts"
  - "**/*.tsx"
  - "**/*.py"
  - "**/*.rs"
alwaysApply: false
---

# Open Responses Agent Development

You are an expert at building autonomous agents using the Open Responses API via the HuggingFace Inference Providers router.

## Core Concepts

### 1. Single Unified Endpoint
All requests go to: `https://router.huggingface.co/v1/responses`
Provider selection is done via MODEL SUFFIX (e.g., `:groq`, `:together`, `:nebius`, `:auto`)

### 2. Request Structure
```json
{
  "model": "moonshotai/Kimi-K2-Instruct-0905:groq",
  "instructions": "You are a helpful assistant.",
  "input": "User's task or request",
  "tools": [...],
  "tool_choice": "auto",
  "reasoning": { "effort": "medium" }
}
```

### 3. Response Structure
```json
{
  "id": "resp_abc123",
  "model": "moonshotai/Kimi-K2-Instruct-0905",
  "output": [
    { "type": "reasoning", "content": "Let me think..." },
    { "type": "function_call", "name": "search", "arguments": {...} },
    { "type": "function_call_output", "output": "..." },
    { "type": "message", "content": "Final response" }
  ],
  "output_text": "Final response text (convenience helper)",
  "usage": { "input_tokens": 100, "output_tokens": 200 }
}
```

## Provider Suffixes
Use model suffix to select provider (single endpoint for all):
- `:groq` - Groq (fast inference)
- `:together` - Together AI (open weight specialist)
- `:nebius` - Nebius AI (European infrastructure)
- `:auto` - Automatic provider selection

Example: `meta-llama/Llama-3.1-70B-Instruct:together`

## Tool Definition Format
Tools are defined at TOP LEVEL (name, description, parameters) - NOT nested in `function`:
```json
{
  "type": "function",
  "name": "get_weather",
  "description": "Get weather for a location",
  "parameters": {
    "type": "object",
    "properties": { ... },
    "required": [...]
  }
}
```

## Reasoning Visibility
- **RAW** (open weight models): Full `content` field
- **SUMMARY** (some proprietary): `summary` field with sanitized reasoning
- **ENCRYPTED** (most proprietary): `encrypted_content` field, not accessible

Control reasoning depth with: `"reasoning": { "effort": "low" | "medium" | "high" }`

## Best Practices
1. Always use the unified endpoint: `https://router.huggingface.co/v1/responses`
2. Select provider via model suffix: `model-id:provider`
3. Use `instructions` field for system prompt
4. Handle all item types: reasoning, function_call, function_call_output, message
5. Use `response.output` (not `items`) and `response.output_text` helper
6. Use HF_TOKEN environment variable for authentication
7. Use OpenAI SDK with custom `base_url` for easiest integration

## SDK Usage (Recommended)

### TypeScript
```typescript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://router.huggingface.co/v1",
  apiKey: process.env.HF_TOKEN,
});

const response = await client.responses.create({
  model: "moonshotai/Kimi-K2-Instruct-0905:groq",
  instructions: "You are a helpful assistant.",
  input: "Your task here",
});

console.log(response.output_text);
```

### Python
```python
from openai import OpenAI

client = OpenAI(
    base_url="https://router.huggingface.co/v1",
    api_key=os.environ.get("HF_TOKEN"),
)

response = client.responses.create(
    model="moonshotai/Kimi-K2-Instruct-0905:groq",
    instructions="You are a helpful assistant.",
    input="Your task here",
)

print(response.output_text)
```

## Output Item Types
```typescript
type OutputItemType =
  | "reasoning"           // Agent's thinking process
  | "function_call"       // Tool call request (name, arguments)
  | "function_call_output" // Tool execution result (output)
  | "message";            // Final response to user
```

## Resources
- HuggingFace Docs: https://huggingface.co/docs/inference-providers/en/guides/responses-api
- Open Responses Spec: https://www.openresponses.org/specification
- Examples: `examples/typescript/`, `examples/python/`, `examples/rust/`
